{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bayesian Data Analysis\n",
    "Bayesian data analysis is a broad framework of analyzing data. It's often compared with the frequentist data analysis where in a naive description the true parameters are fixed and data are random and we ought to find the right inference methods to inference for the true parameters. In contrast, in Bayesian data analysis, we think the data is observed and therefore fixed, and given data, the parameters are changing according to some distributions. The Bayes theorem, in the form of a simple formula, provides a framework of carrying out the Bayesian analysis.\n",
    "\n",
    "#### Bayes Theorem\n",
    "The Bayes theorem states the conditional relationships of two random variables in the form as:\n",
    "\n",
    "$$P(X|Y)=\\dfrac{P(Y|X)P(X)}{P(Y)}$$\n",
    "\n",
    "Here in Bayesian data analysis, Y refers to the data observed, X refers to the parameters in the data generating process producing Y. If we make the notations explicit, then without loss of generality, suppose $y_1, y_2,\\ldots,y_n$ is the univariate value of a sample of $n$ observations, and $\\theta$ is the parameter of the **data generating process** that we'd like to make inference for, then by Bayes theorem, we can have \n",
    "$$P(\\theta|y_1,y_2,\\ldots,y_n)=\\dfrac{P(y_1,y_2,\\ldots,y_n|\\theta)P(\\theta)}{P(y_1,y_2,\\ldots,y_n)}$$\n",
    "\n",
    "If we assume the observations are i.i.d., the component $P(y_1,y_2,\\ldots,y_n|\\theta)$ is essentially the likelihood function of the parameter, i.e., $P(y_1,y_2,\\ldots,y_n|\\theta)=L(\\theta)=\\Pi_{i=1}^np(y_i|\\theta)$. The denominator $P(y_1,y_2,\\ldots,y_n)$ is the unconditional probability of the observations which is a constant since the observations are observed, and therefore is usually omitted in the calculation. The component $P(\\theta)$ is the *prior* distribution of the parameter $\\theta$ which described either the information collected before the experiments or experts' judgement or knowledge about the parameter. Sometimes for the convenience of modeling, conjugate prior distribution is adopted such that the posterior distribution would be a well-studied distribution. The posterior distribution is then written as\n",
    "\n",
    "$$P(\\theta|y_1,y_2,\\ldots,y_n)\\propto L(\\theta)P(\\theta)=\\Pi_{i=1}^nP(y_i|\\theta)P(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### One example\n",
    "Now suppose we observe a random sample of $n=100$ observations and we'd like to model the d.g.p. as a normal distribution with mean $\\mu$ and variance $\\sigma^2$, i.e., $\\theta=(\\mu, \\sigma^2)$. We then specify the prior distributions of $\\mu$ as $\\mu \\sim N(0, 10)$ and $\\sigma^2$ as $\\sigma^2 \\sim \\text{Inv-Gamma}(a, b)$, where $a$ and $b$ are hyper-parameters for the prior distribution of $\\sigma^2$. \n",
    "\n",
    "Then $$L(\\theta)=\\Pi_{i=1}^n\\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{(y_i-\\mu)^2}{2\\sigma^2}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
